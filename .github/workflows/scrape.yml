name: scrape-news
on:
  schedule:
    - cron: "10 11 * * *"  # 06:10 Bogotá = 11:10 UTC
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Run scraper
        env: { PYTHONUNBUFFERED: "1" }
        run: |
          set -euo pipefail
          echo "== Running scraper =="
          python scripts/scrape_news.py \
            --outdir data \
            --pages 2 \
            --max-per-factcheck 150 \
            --max-per-trusted 60 \
            --target-total 300 \
            --ratio "falso=0.4,verdadero=0.4,dudoso=0.2"

      - name: Show data tree (after scraper)
        run: |
          echo "== ls -R data =="
          mkdir -p data
          ls -lah data || true
          echo "== find data =="
          find data -maxdepth 3 -type f -printf '%p\t%k KB\n' | sort || true

      - name: Consolidate history
        run: |
          set -euo pipefail
          python scripts/consolidate.py data data/dataset_historico.csv

      - name: Clean historical (mojibake + listados)
        run: |
          set -euo pipefail
          if [ -f "data/dataset_historico.csv" ]; then
            python scripts/clean_historico.py data/dataset_historico.csv data/dataset_historico_clean.csv
          else
            echo "[SKIP] data/dataset_historico.csv no existe todavía."
          fi

      - name: Validate & report
        run: |
          set -euo pipefail
          python scripts/validate.py data data/reportes

      - name: Upload artifacts (jsonl/parquet/reportes)
        uses: actions/upload-artifact@v4
        with:
          name: datasets-${{ github.run_id }}
          path: |
            data/jsonl/*.jsonl
            data/parquet/*.parquet
            data/reportes/*.md

      - name: Verify outputs exist (fail if missing)
        run: |
          set -euo pipefail
          test -f "data/dataset_$(date -u +%F).csv"
          echo "OK: dataset diario existe."

      - name: Commit results
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: daily scrape + consolidate + report"
          file_pattern: |
            data/*.csv
            data/jsonl/*.jsonl
            data/parquet/*.parquet
            data/reportes/*.md
