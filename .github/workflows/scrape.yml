name: scrape-news
on:
  schedule:
    - cron: "10 11 * * *"   # 06:10 Bogot√°
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    permissions: { contents: write }
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Run SAFE scraper (fact-checkers only)
        env: { PYTHONUNBUFFERED: "1" }
        run: |
          set -euo pipefail
          python scripts/scrape_news_safe.py \
            --outdir data \
            --pages 3 \
            --max-per-factcheck 400 \
            --target-total 300 \
            --ratio "falso=0.4,verdadero=0.4,dudoso=0.2" \
            --min-title 12 --min-text 400 --require-date 0

      - name: Show data tree
        run: |
          mkdir -p data
          ls -lah data || true
          find data -maxdepth 3 -type f -printf '%p\t%k KB\n' | sort || true

      - name: Consolidate history
        run: |
          python scripts/consolidate.py data data/dataset_historico.csv || true

      - name: Validate & report
        run: |
          python scripts/validate.py data data/reportes || true

      - name: Verify outputs exist (hard fail if missing)
        run: |
          set -euo pipefail
          test -f "data/dataset_$(date -u +%F).csv"
          echo "OK: dataset diario existe."

      - name: Commit results
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: SAFE scrape (fact-checkers) + consolidate + report"
          file_pattern: |
            data/*.csv
            data/jsonl/*.jsonl
            data/parquet/*.parquet
            data/reportes/*.md

